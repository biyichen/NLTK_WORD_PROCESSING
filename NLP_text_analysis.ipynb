{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/biyichen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tokenization process means dividing large parts into widgets. use NLTK tokens to split the text. This is something you might think, it's too simple, you don't need to use NLTK's tokenizer, you can use regular expressions to split sentences, because each sentence has punctuation and spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then look at the following text: 'Hello Mr. Adam, how are you? '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So if you use punctuation to split, 'Hello Mr','Adam','how are you' will be considered each individual sentence, but if you use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Could also split the sentence into words. The word Mr. has not been separated. NLTK uses the PunktSentenceTokenizer of the punkt module, which is part of NLTK.tokenize. And this tokenizer is trained to work in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n"
     ]
    }
   ],
   "source": [
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-English Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(mytext,\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the nltk.download() installation interface, one of which is WordNet. WordNet is a database built for natural language processing. It includes some synonym groups and some short definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn = wordnet.synsets(\"pain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use WordNet to get synonyms like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "  \n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antonym processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "  \n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"small\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In language morphology and information retrieval, stemming is the process of removing the affixes to get the roots. For example, the working stem is work. Search engines use this technique when indexing pages, so many people write different versions of the same word. There are many algorithms to avoid this, the most common being the Boolean stem algorithm. NLTK has a class called PorterStemmer, which is the implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "  \n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "print(stemmer.stem('worked'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-English stem extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In addition to English, SnowballStemmer also supports 13 languages. Supported languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "  \n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word variant reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word variant restore is similar to a stem, but the difference is that the result of a variant restore is a real word. Unlike stemming, when you try to extract certain words, it produces similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "  \n",
    "stemmer = PorterStemmer()\n",
    "  \n",
    "print(stemmer.stem('increases'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, if you use NLTK's WordNet to perform a variant restore of the same word, the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "print(lemmatizer.lemmatize('increases'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result may be a synonym or a different word of the same meaning. Sometimes when you restore a word to a variant, you always get the same word. This is because the default part of the language is a noun. To get a verb, you can specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### In fact, this is also a good way to compress text, and finally get the text from the original 50% to 60%. The result can also be a verb (v), a noun (n), an adjective (a), or an adverb (r):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difference between stems and variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "  \n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(stemmer.stem('stones'))\n",
    "print(stemmer.stem('speaking'))\n",
    "print(stemmer.stem('bedroom'))\n",
    "print(stemmer.stem('jokes'))\n",
    "print(stemmer.stem('lisa'))\n",
    "print(stemmer.stem('purple'))\n",
    "print('----------------------')\n",
    "print(lemmatizer.lemmatize('stones'))\n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    "print(lemmatizer.lemmatize('purple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
